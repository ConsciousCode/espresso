Here are some clarifications for the stages of development for espresso

Espresso as a language is designed to be a highly optimizable high level language. Its syntax is based around generalizing existing patterns in order to simplify the grammar: simple rules, arbitrary complexity.

Part of both optimizability and simplicity is bootstrapping its own compiler. This way, its standard library serves as the defacto implementation written entirely in itself comparable to PyPy. Unlike PyPy however, we're designing the language from scratch so we don't need metatracing to make optimization possible.

This leads to strict code which is identical in syntax, but different in semantics. It doesn't allow duck typing, all operations must be unambiguously resolvable to single operations. Strict code is intended to provide an interface for low level highly optimizable code ala RPython or asm.js. It's opt-in, and its semantics are sufficiently annoying to discourage unwarranted use. It should be trivial to transpile strict espresso code into C (with the potential complexity of having to implement the runtime library).

Because we have this restricted subset, the compiler can be written in strict code and bootstrap itself, and this is compilable to native code. Some kind of interpreter is required for initial execution, but then it can execute itself. This implementation need only be sufficiently complex to interpret the base of the compiler. Obscure features, edge cases, and even robustness aren't a real concern

We ought to make an IR targeted for espresso which supports both the high and low levels. This should decouple assemblers/JIT/interpreters from the parsing stage

https://www.youtube.com/watch?v=ARYP83yNAWk
* Possible change to C++ exception model
* Static exception types transparently use Result types ala Rust
* Key takeaways from a philosophical POV:
  - not all errors are the same
  - state corruption (eg stack overflow, null pointer deref) - unrecoverable, panic and maybe store some state to HDD
  - programming bug (eg failed asserts, index out of bounds, contract violation) - still unrecoverable, report to programmer
  - recoverable error (code was unable to complete its task due to some rare but expected issue) - runtime exceptions

https://www.youtube.com/watch?v=vElZc6zSIXM
* How to make fast small data structuress
* Custom containers which store their data in the structure
* Lets you alloc on the stack and return by value
* For lists of big objects, small vector of unique ptrs (branch prediction+++)
* Pointer-like with embeddable ints in leftover bits
* Variant types between pointer and pointer to vector of pointers w/ pointer size

AsmJIT project (C++ dynamic assembler ala DynAsm without preprocessing)

fbstring
* small string representation
* small strings are 24 bytes on the stack
* Last byte is the capacity
* When capacity is 0, it doubles as null terminator

? meta-operator "optional"
eg: return? throw?
	op ? operand = if(operand) op(operand)
Overloaded semantics
	k :? v meaning k:v pair if v is truthy
	obj .? val meaning obj.val only if val is truthy
* The whole expression containing ? is unevaluated if its operand is falsey
  - eg x = x.?next translates to if('next' in x) x = x.next
  - this might be less useful, maybe subexpression semantics is to replace with nil?

Assignment operator x := y -> Object.assign(x, y)
Nullish coalescing operator ??
* Same semantics as optional meta-operator, but using nullish instead of falsey

= meta-operator "inplace"
Rather than defining multiple in-place operators, they're defined in one go using a meta-operator = which generalizes to all operators, eg === = is inplace id equality

x::y(z, ...args) = x.y.call(z, ...args)
* eg Array::concat(arrayLike)

x->y == (t = symbol("tmp"); x[t] = y; x[t] after delete x[t];)

Basically access y as if it was a property of x. This has two primary uses, generic function application and accessing "private" members (which normally require this.y)

Going with the first idea, the parser could only understand ASCII and leave translation to the ZIO abstraction. That way, we could do stuff like whitespace -> space, newlines -> \n, etc so the parser never has to check more than those basic characters. It also adds flexibility to the source of the buffer stream, which can be encoded any which way so long as it boils down to ascii. To enable this, we could also have some kind of parser-only identifier escape to support name mangling.

Ok, that gives us the following schema:
* ZIO acts as a buffered stream abstraction which is accessed like an ASCII string
* ZIO implementations can add conversions from different encodings to a canonical ASCII representation
* This gives us a restricted ASCII subset of:
  - \n, 0x20:0x7F
  - Keep $ as a start character like JS

Actually, better idea: Abuse a Lua-like ignorance of extended ascii, using 0x80, 0x81, and 0x82 as DLE, SO, and SI respectively. This has the benefit of a much simpler ctype check and takes care of embedding UTF-8 directly rather than trying to convert to hex. According to the UTF-8 spec, these characters can only occur in continuation bytes anyway, and so should be invalid according to UTF-8 rules normally.

Because of all this, the core parser can act like a simple state machine on an ASCII codeset and has infinite extensibility wrt actual encoding using a stateless character conversion. Note that this filtering mechanism has to be implemented in esp_Reader, as it would otherwise hamper ASCII reading with an extra function call per character.

Let's get a core feature list:
* Monomorphic functions (possibly with hidden polymorphism ala V8)
* Prototype-based inheritance system
* Language-level support for native code interfacing (eg binding native functions/objects directly with small wrappers)
* Use the prototyping system to make the interpreter threadsafe by having the innermost behaviors readonly, while overrides are thread-local
* Do the same (by default) for imports, but without freezing the module - allows local mutability which doesn't affect the module, and monkey-patching if needed
* In general, default behavior should tend towards static and type-safe, with monkey-patching allowed with great potential deoptimization costs
* Base grammar should be LL - semantic layer can be more complex
* Reuse keywords where possible
* Parser operates on ASCII using a custom extension - unicode can be supported by supplying UTF-8, so long as 1. whitespace is converted to ' ', 2. newlines are converted to '\n', and 3. identifier start characters are prepended with '\x7F' (which will be otherwise ignored)
* Get rid of old legacy features no one uses: many string escapes, 0-prefixed octal, unsigned integers, special handling for control chars (eg isspace(x) = x <= ' ')
* Main idea is to live up to the name: Espresso is all the best features in the smallest/simplest package.

To help with operator overloading, it's probably a good idea to only support the base ops and not assignment ops (which add a whole layer of complexity and - conceptually speaking - are redundant, since inplace optimizations can be approximated using lazy evaluation). Similarly, we earlier decided to support all the comparison ops rather than extrapolating from the spaceship operator because it would imply strict ordering, but it's odd to add complexity to AVOID supporting something. If someone really wants an error they can throw one, otherwise whatever garbage value is returned is better than excess complexity.

GC reference type:
* Global
* Eternal - globally allocated, never destroyed, mutable
* Native bindings - Dynamically created but immutable afterwards
* VM data (stack preallocation)
* Static size (eg tuples)
* Old memory
* Dynamic memory
* Raw buffer (unaligned)

Values with only one reference are prime candidates for relocation 

Nomenclature:
* Simple value = contained by the tagged union/nan box with no indirection
* Managed value = simple value represents a pointer to GC-managed memory
* Fixed = size of an underlying structure cannot change
* Immutable = fixed and the data cannot change
* Mutable = size and data is capable of changing
* (Note: there is no unfixed immutable data as that is a contradiction)
* Bare = structure is allocated outside GC-managed memory and must collect its own garbage 
* Native = something unrelated to the GC-managed memory

GC:
* Simple types - none, bool, int, real, and empty objects
* 4 basic pools - mutable bytes, immutable bytes, mutable object arrays, immutable object arrays
* Mutable types always have a level of indirection, meaning they can have bare instantiations on the stack pointing to managed memory - eg the esp_Buffer structure has a size, capacity, and pointer to memory so that mutations of the underlying pointer won't change references to the object.
* Immutable types don't have to worry about the underlying pointer changing so they don't need any indirection. (Note: not strictly true if we use a compacting GC) Structures like esp_String contain their data in-place and cannot be instantiated on the stack because they have no fixed size.
* This has implications on the GC and how it interacts with native code - if we have a bare type on the stack, it obviously can't be garbage collected, and the data it points to can't either. The data it points to will never outlive the object itself, but may be replaced for reallocs. Thus we can say the object "owns" its data, and set the maximum lifetime to the object's lifetime.
* Stack-allocated bare objects have their underlying pointer within managed memory which is not garbage-collected. Instead, the onus is on the native code to free its memory when the object becomes unreachable.
* Similarly, bare immutable objects don't have a well-defined lifetime according to the GC and must be freed by the native code. In a manner of speaking, the pointer acts as their bare value

Interesting ideas:
* Unwrap using invert/negate and an all-ones tag
* Use the fact that signed integer wrapping is generally undesirable to increase code space - cases where it would happen naturally can be made rigorous with a much more costly operation
* Possibly use the fact that two's complement positive is unchanged but negative is inverted
* Bias positive integer encoding over negative
* Use float-encoded integers literally?
* Bottom 3 bits can be used for tagging, not just literals

We have two separate types for char vs string, particularly since char is easily represented as a simple value. This lets us encapsulate some of the weirder Python methods like isdigit() on normal strings and a chr/ord distinction. chars act like a special mixture of string and int. For instance, string has string*int = string.repeat(x) but char*int results in an int. "a" + 1 = "a1", "a" + char('a') = "aa". On reflection, I won't bother having char literals because it has a lot of possibilities to mess things up (eg ' '*10 is a commonish Python idiom, but if that were a char literal it would produce nonsense). Instead we can do things like s[x].isDigit() which doesn't do weird Pythonic things like check every character (or maybe it can still do that for strings, but without iteration for chars)

[...each(var x in y) x*2] = [...(function yield() { each(var x in y) yield x*2; })()]

Prototype chain lookup optimization: prototype is in the shape, not the object
* Also: use IC to store which prototype had a method, requires a way to poison the IC (v8 uses a "ValidityCell" property on prototype shapes)

Prototypes could be made immutable without changing much, but I think making prototype mutations possible adds a lot to potential expressiveness. JS usually implements this by making it very expensive - but we could instead add an extra layer of protection with data flow design making it harder to accidentally mutate the prototype. Eg, importing a module actually prototypes the original object, so changing its properties globally requires .proto first

async function test() {
	await async.sleep(1000);
}

test();

1. start a new greenthread (push function context to the thread scheduler)
2. original context keeps executing, then eventually switches to test
3. test executes sleep(1000), then synchronizes with it (ala fork join)

Dart uses a split concurrency model which streamlines the JS model - that is, single-threaded cooperative multitasking via a round-robin scheduler and event loop and true multitasking using isolates.

That being said I think mimicking the JS concurrency model is ideal, possibly with some minor enhancements (eg empty await to yield control arbitrarily ala nextTick or language-level support for Promise.all/Promise.race). Particularly, it's significantly lower complexity than true greenthreads with roughly the same power

---

JS actually might already resolve this problem internally, because afaik accessors are inheritable, which shouldn't be possible normally. One idea I had while trying to sleep was to have a property flag which says "never use own key", maybe "delegated"

Also thought on closed vs open namespaces, naively they could both be implemented with ordinary objects. In a closed namespace, the dictionary would act as metadata, whereas in an open namespace it provides arbitrary extension. However, there's some key notes: first, the closed top level namespace of a module can be GC-pruned once it finishes executing, as if it were a function scope. Any globals captured by closures would be contained by upvals and thus remain live. So basically, closed namespaces only make sense in the context of an actively executing scope

Open namespaces on the other hand implemented using objects is also problematic, because objects are optimized for defined-at-instantiation properties with support for arbitrary extension. An open namespace is expected to grow arbitrarily, so it would end up using just the dictionary. Since we're using contiguously allocated value arrays for dictionaries, it shouldn't be too difficult to transition this to act as if it were the slots of an object, thus allowing lookups by offset

V8 invalidates the IC by keeping a reference to a boolean "validity cell", sort of like a weak reference. If it's false, the IC is flushed and slow prototype lookup is done again. This isn't too bad for V8 which creates new objects for every shape change, so the prototype/shape which is invalidated is trash. As a possible optimization for our use-case, we can use an int instead of a boolean which increments with each change to get a "version" token. If the versions differ, redo prototype lookup. This leaves an edge case, where the int would overflow - at this point we can stop incrementing the version and create a new object like V8 because if we reuse lower values, there's a (very small but nonzero) chance that a rare code path still has an older version

V8 has a GC optimization for arrays: if all elements are inline values, there's no need for GC to scan them. eg if it's all ints, these aren't GC managed

GC types:
* Mark and sweep and invert (when grey set is empty, collect condemned set and invert white/black meaning)

We have a clear distinction between static and dynamic memory - whether or not the size can change.
* Dynamic
  - Buffer (size + capacity + ptr(data)*1 = 16)
  - List (size + capacity + ptr(data)*8 = 16)
  - Dict (proto + ptr(keys)*8 = 16)
  - Long (variable*8...)
* Static
  - String/bytes (size + data*v...)
  - Objects (shape + slots*8...)
  - Tuple/array (size + data*8...)
  - Wrapped (shape + schema*1...)
  - Function (ktab + code*1...)
  - Closure (func + upvals*8...)
  - NFunction (func + schema*1...)
* Inline
  - None/bool
  - Int/real
  - Char/smallstring
  - Opaque
  - CFunction

Key difference between dynamic and static memory (other than resizeability) is that dynamic memory requires a constant-sized structure with a pointer to the dynamic part. This is because static memory never has to realloc its data so we can use the data (with a header) directly, whereas dynamic memory has to realloc and if we changed the reference every time the buffer moved, we'd have no way to update every reference to the object. As an interesting aside, this implies that (in general, some exotic objects may break this) the dynamic portion of a resizeable object has exactly one reference at any given time, suggesting it doesn't need to be GC'd - although any children would be

Some ostensibly dynamic objects can be implemented as if they were static, eg long objects because they're defined to never have more than one reference (assignment copies memory)

Compacting GC observation: if we do a complete scan of reachability, we will eventually visit every node and thus have the opportunity to update their references. One small bit of complexity is we need a way to distinguish a redirect pointer

Possible idea: Continuous generational compacting GC
* Try to keep older objects in lower memory
* Don't have explicit generations, just an age gradient
* Aim for eventual compaction - we don't care about compacting everything all at once, we can compact individual objects (problem: we no longer have a guarantee that we'll visit all objects and update references eventually)

Is there any way to remove subgraphs from GC consideration? Like when a number of references are predicated on a single reference - at the very least, this could apply to uniquely-held upvals which can't escape, difficult to prove - actually no, impossible because anything you can do to the upval could call arbitrary code

Idea: Dispense with the idea of extending objects entirely, leaving that up to dictionaries. Objects have a fixed size, and can be extended by using them as a prototype for a dictionary. This would have some level of consistency, since we already aren't allowing lists to be extended. This is kind of weird though, right? Objects have to have a shape dictionary to account for late (untyped) binding which we were using as a CoW metastructure, which enables promotion to dynamic objects. Actually, that has zero overhead unless you use it and adds a ton of flexibility.

Lua GC notes:
* Keeps all GC objects in a linked list
* Tricolor mark and sweep
* Generational: young, old (+2 minor sweep collections)
* Minor sweep: only traverse young objects
* Generational invariant: old can't point to young
* If old points to young, added to "touched" list (also swept in minor phase)
* Mutating a black object requires either forward or backward color: table goes black -> grey, or object goes white -> grey
* Assignment to table goes black -> grey (best for container objects which tend to get several stores in succession)
* Assignment to metatable moves white -> grey (best for objects with isolated stores)
* Tables which are demoted back to gray added to special list for an atomic phase
* Stack kept grey until the very end

http://wiki.luajit.org/New-Garbage-Collector
LuaJIT GC:
* Request memory from OS called Arenas, split into 16-byte cells
* 1+ cells makes a block
* All objects in an arena are either traversable or not
* Block and mark metadata separated from data
* Huge blocks kept in separate arenas
  - Always multiples of arena size
  - No header, metadata stored in a hash table
* Quad-color local to an arena
* Newly allocated objects are grey-white, write barrier can check for just !grey
* grey-black maintains a stack, grey-white can be freed as if it were white
* Grey bit seems to act like a "dirty" bit
* Modified generational GC (triggered by high death rate for young allocs)
* Don't flip black -> white before a minor collection
  - Only newly allocated blocks and dirty objects are traversed, pure black is assumed to be reachable (regardless of mutation)
* Cell index can be derived from starting address by >>4 & LSB
* Cell index always fits in 16 bits
* Two bitmaps determine arena block status, block and mark
  - Block=1 => First cell of a block
  - Mark = white/black or padding/free
* Allocator modes switch depending on fragmentation pressure
  - Bump allocator (increment pointer)
  - Fit allocator (fill holes)
    * Setup bins for size classes using multiples of cell size -> powers of two
	* Each bin is the anchor for a list of free blocks for that size class
	* Bounded best-fit allocation first, fall back to first-fit
	* First-fit can adapt, high miss rates => higher size classes searched first, high hit rate => smaller size classes
* Write barrier: if(!grey) grey = 1 and if(black) push to sequential store buffer (SSB)
* Mark bit only needs to be checked during mark phase, if collector is paused no check (since there's no black objects)
* SSB buffer holding block addresses which triggered write barrier (must have at least one free cell)
* SSB overflow -> convert addresses to cell indices and push to grey stacks
* Use of SSB prevents the cache from being thrashed by GC data while the mutator is running
* Grey stack: per-arena, cell indices of dark-grey blocks 
* Objects removed from grey stack are turned black before traversal
* Current arena processing continues until grey stack is empty
* Grey queue: priority queue of arenas with non-empty grey stack ordered by stack size
* Sweep phase has good cache locality because metadata for blocks is kept together. Don't have to traverse the actual data, just the metadata
* Bitmap tricks allow bitwise parallel operations
  - Major sweep: block' = block & mark, mark' = block^mark (free white blocks and turn black blocks into white blocks)
  - Minor sweep: block' = block & mark, mark' = block|mark (free white blocks, preserve black blocks)
  - Compare block and mark word gives status of last block in the word, eg block' < mark' => last block is free
* ---
* Write barrier triggered when an object with grey=1 is written to, and thus it needs to be revisited
* Grey seems to correspond to "dirty", whether or not it needs to be revisited
* White = maybe garbage, Black = probably not garbage
* GC sweeping is done per-arena, which at first appears to break inter-arena references. But in fact, if a major sweep is done first this will mark any objects referred to in other arenas as black, and they will remain black for each minor collection. Then, a major collection marks everything as white and traverses all arenas. The key here is not resetting black -> white between minor collections

Moving GC with conditional indirection?
* If an object is moved, replace it with a pointer to its new location marked "moved"
* If the mutator accesses it, it updates its reference to the new location
* Because the mutator never propagates the old address, we can guarantee that by the end of a full sweep the old address is garbage
* Alternatively, we could consider relocators to be specially-treated parts of accessibility analysis, and if they fail to be marked then they're garbage automatically

ZGC:
* Uses a forwarding table mapping old -> new address
* Map only checked if the reference has a "bad color"
* Remap bit set during mark phase, meaning "this object will be relocated by the end of this phase"

My GC:
* Quad-color mark and sweep kept arena-local for minor sweeps
* Lazy compaction - if an object with the right size for a hole is found, copy contents to the hole and replace the original object with a pointer to the new address. Any accesses check the MSB for 0, and perform a second dereference and update their state. This forward-pointer is then eventually collected as garbage when all references are updated
  - Problem: Needs a grey bit to participate in GC, right?
  - Except, it can be considered as having "no children", so any access marks it black and the grey bit is redundant
* Quipu structure to keep track of the holes
* Prefer exact fit to bump
* Need a way to recover the bump block

Currently, deallocation pushes to the quipu. We can check deallocations for adjacency to the bump block, then trigger a bump recovery checking adjacent empty blocks. This could potentially conflict with the LIFO policy of the quipu, where old objects are allocated in smaller addresses hidden by later allocations, and new allocations prefer objects deallocated "recently" (ie closer to the bump block). The list is being grown, so we could keep it sorted in O(n) time - but this would require higher cache invalidation

For now, let's go with the "good enough" approach. Keep the quipu structure as-is, do a quick check for bump adjacency, and if adjacent rebuild the bump pointer (basically ctz). To make this work, we need a way to determine from the last cell of an empty block how big it is. Easy, make blocks have a minimum size of 2 and set the last cell to a previous link. This gives us the size (rev->next - &rev) and O(1) updating

Apparently V8 can get equal performance with pointer compression, using effectively half the memory. I want that. JerryScript goes even crazier by supporting 16 bit cptrs by default. Considering the level of repetition between the primitives, I think I'll try to abstract it one level further to a lower level

Pointer always points to GCObject
Bytes is an arbitrary number of bytes
Array is a fixed-length sequence
Table is a hash:int hashtable which can be reused
Func uses opcodes
CFunc uses a standardized format
Closure contains upvals - only valid for Func

def y.x = {
	get() {},
	set() {},
	delete() {},
	define() {}
}

Base design of the variant type on 4-byte words
* On 32-bit platforms pointers are unencoded
* On 64-bit, use cell-relative addressing (vs arena-relative addressing which uses a global address which may not be in cache. Many of the issues with cell-relative addressing are resolved simply by 1. only allowing references to objects, not cells and 2. having a heap-allocated wide pointer for references outside the range)
* This avoids excessive waste, as normal scripts shouldn't exceed even a few MB, much less 4 GB.

String optimization used by V8 and SpiderMonkey: ConsString, 2-tuple of strings which are concatenated forming an ad-hoc tree/rope. When an indexing operation is performed, it's flattened. SpiderMonkey also uses substring views to make mixing building and indexing operations less harsh. eg c = a + b, if the backing store for a is large enough to contain a + b, c inherits the backing store and a becomes a substring view into c. Considering we're aiming to replicate Pythonic slices this is probably something easy to get without too much complexity

GC Policies:
* Strings - one arena for all strings with a single lookup table. Hashes and GC data are stored in the lookup table, optimized for string equality and hash lookup. Uses a bump allocator, deletions can move the strings at will because they're referred by lookup
* Sequences - Binary tree splitting the entire arena so realloc is almost always free. Always favor the largest contiguous block. Sequences aren't referred to directly, but rather by a corresponding primitive - each gets a reverse reference to its referer. As a result, these are very easy to relocate no matter how big they get. If more memory is needed, we can relocate based on how long ago it was used last (splay tree?) 
* Primitive - Bump allocator + quipu to keep track of free blocks. Objects tend to be small and word-aligned

The sequence policy is optimized for many small variable-length arrays or few larger arrays. If a larger size is requested than can be supported by an arena, we can either allocate another arena or "give up" and use plain malloc/realloc. Might also have a mechanism for "freezing" and "unfreezing" sequence types, which move them to/from the sequence pool. This could be implicit too, to free up sequence memory as needed

Type inference:
* In methods we can automatically assume the type of `this` which short-circuits the slow prototype chain lookup path in favor of direct slot indexing. This isn't guaranteed, because methods are ordinary functions and can be called with foreign types, but it allows us to provide a specialized version
  - Specialized bytecode has to be thrown away if the prototype is changed
* Support TypeScript-like type annotations, functions are still defacto monomorphic. I keep flip-flopping on implementing this, but considering both Python and JS (via TypeScript) are going with type annotations, and the fact that it gives me anxiety imagining not having type information readily available when I want it to be available, it'd be dumb not to. Also even Python supports generics

func::() returns a generic function
func::(int) returns a function with the first parameter specialized on int, etc

Example:

function add(a: int, b: int) { return a + b; }
add::()("string", 0.1) == "string0.1"

Flags:
* Here = unset if this is a forwarding pointer
* Moved = This object was moved, don't move again until the forwarding pointer is freed (which unsets this bit)
* Dirty = Dependents need to be checked
* Unique = Know that there's only one reference to this (might be a micro-optimization)
* Stack = Object only has stack references

Types of handles:
* LiveValue - Word-sized, actively used by the VM
* HeapValue - 4-bytes, (maybe) compressed values within the heap
* Var - LiveValue which has its address added to a GC root list

32-bit, LiveValue = HeapValue because word-size = 32-bit and HeapValue uses absolute addresses
64-bit, LiveValue is absolute while HeapValue uses object-relative addressing

64-bit LiveValue has a lot more bits to work with, so we can store more values inline.
* Can use NaN boxing to encode float
* C string
* Char
* Int

Strings are split between two different cases - interned, and normal. Interned strings are expected to never go away and include documentation, metadata, and keys. They're specifically for cases where we don't care about the contents of the field, only the identity. Normal strings are for everything else, from ordinary string literals to concatenations and formattings. Basically anywhere we're dealing with the contents and not the identity.

Idea: If string interpolation is implicit, and bytes can just be a normal constructor, we can use backticks to represent raw (unescaped) strings and no longer have any flag parsing

Idea 2: Have a special operator (";"?) which is called when a value appears at the top level of an expression (and isn't the result of an assignment)

Primary semantic difference with strict mode is the any type is explicit, and no operator overloading

Prototype languages purposefully eschew formal typing and the type hierarchy is dynamically generated. OOP-style ownership of methods for single dispatch thus makes sense. When you try to generalize to multimethods though, none of the parameters own the function. The behavior is entirely defined by their runtime type. Slate gets around this by inverting the ordinary relationship, where each prototype is given ownership of a "role" when the method is defined, eg "I can implement role 2 with these methods" and from this we can derive a set of applicable methods. This is a clever hack, but it inverts the intuitive conceptual model and spreads multimethod implementation across all objects, even if they don't specialize a method

roles = [
	{int: [method1, method2]},
	{number: [method1], bool: [method2]}
]

roles[0][int] = {method1, method2}
roles[1][float] = {}
	float -> number
roles[1][number] = {method1}
intersect: {method1}

(int, number) => method1()
(int, bool) => method2()

This is pretty good actually, the dispatch follows an O(n^2) sort of pattern. It leans more heavily on the left but makes the algorithm easy. Walk down the prototype chain until we find a match, then move to the next role, walk down the prototype chain, and if the intersection is empty return to the first role. Greedy left-biased first-fit. Also, matching prototypes is done via exact match

Actually, we can make that roughly O(np) (n = arity, p = prototype depth) if we iterate the prototype chains all at once. If any of the roles match the type, lock that role to those methods and continuing iterating the prototype chains

Iterate all the roles and find candidate matches for each layer . Then, do a set intersection and the result is the set of applicable methods. Finally we need a total ordering

Conceptualization: (bedsheet dispatch?) Draping a blanket over an object. The blanket is the breadth-first check for exact values, if a point matches somewhere that has a higher precedence than any other matches. The first exact match (excluding the role) wins. This generates an ordered list of candidate methods because traversing the prototype chain will always be most to least specific

To make this consistent with functions which ignore extra arguments, the formal parameters are followed by infinite any parameters. Because any is the least specific type in the type system, it adds no specificity information and can be ignored.

roles = {[type]: method[]}[]

function dispatch(roles, types) {
	var candidates = [];
	types = types[];
	if(types.length > roles.length) types = types[:roles.length];
	
	loop {
		if(types.all(. == any)) break;
		
		for(var i in ..roles) {
			var r = roles[i], t = types[i];
			if(t === any) continue;
			
			for(var method in r[t]) {
				# The most specific method is found when all types are
				#  satisfied
				candidates.push((method, [false]*types.length))
			}
		}
		
		
		for(var (method, satisfy) in candidates) {
			for(var i in ..types) {
				var t = types[i];
				
				if(method.signature[i] === t) {
					satisfy[i] = true;
					if(satisfy.all()) {
						return method;
					}
				}
			}
		}
		
		# Iterate up the prototype chain by one (any => none)
		for(var i in ..types) types[i] = types[i].proto;
	}
}

I think I should include Julia's parametric types in espresso as syntax sugar - they're functionally identical to a higher-order function returning a dynamically created class (maybe with memoization), but a lot of edge cases have to be handled as well like implementing it as an array indexing, disabling setting, disabling function call as the parametric type is abstract, etc

---

Equal as a meta-operator (not used to build eg == and !=, it still applies for those as "== =" etc)

Still deliberating on macros and giving AST access to code - but I'm thinking to not support it because it's a very niche use for mostly esoteric semantic reasons which could potentially disable optimizations and make the language more complex

use ref[T] which dereferences with val[], which can have semantics outside strict code - that is, a mutable reference value which can hold immutable values. It's also trivial to implement with operator overloading, as .set is the only operator overloads which work with assignment

Strict code has a number of special types not available outside strict code. i8, i32, etc and any, a type representing boxed values which must be explicitly unboxed, and the actual implementation of that is platform-dependent. We want to be careful what implicit operations we allow - type promotion is fine because it's roughly a no-op, but not int/float conversions or anything related to strings. The data model of objects should also be entirely explicit, which allows in-language implementation of a meta-object model with generic interfaces for JIT and GC.

"as" operator reserved for strict code only (at least, it doesn't make sense to use it in non-strict code) - any kind of no-op type promotion, but mostly for unboxing any or union values.

Add ! and ? for rest identifiers to make it more lispy?

the familiar of zero
brave story
now and then here and there
combatants will be dispatched
magical shopping arcade abenoboshi

x <: y = x(y)
x :> y = y(x)
x -> y(...args) = y.call(x, ...args)
Meta operators:
!x x= ..x x.. \x⃝

x \op y = :"\\"(op, x, y) 

Base memory footprint:
* Lua = 20 kB / 10 MB?
* Python = 10 MB
* V8 = 34 MB

https://github.com/nicolasbrailo/cpp_exception_handling_abi

none[x] = none
bool(none) = false
string(none) = "none"
none(...) = no-op - note, it's a lot like JS void

Parser design constraints:
* ZIO: must be stateless, no feedback from later stages and no partial parsing
* Lexer: Generates one token at a time, no lookahead, and no feedback from later stages (eg no regex syntax, which requires knowing the expected next token type). Stack depth is shallow and error conditions are handled by return
* Parser: Generates an AST using a lispian data structure. Stack depth is deep and requires longjmp for error conditions. Memory is allocated on the AST which has a root node, so it can all be freed no matter how deep. Avoid large-scale AST optimizations, only local

Destructuring syntax implemented naively would have infinite lookahead, which is unacceptable for the language's design. A much better solution is to parse a destructuring expression as an object, keeping track of whether or not it's a valid lvalue, and deciding after the following token whether or not it's going to be used as one. A similar issue is encountered with arrow functions. If we parse it like JS, we don't need lookahead per-se, but we still need an intermediate structure which is converted at the end to either function parameters or a parenthesized tuple. However, if we use the same construction as destructuring this becomes similarly trivial - the only side effect is that it's more permissive than JS eg:
* x, y => x + y
* (x, y) => x + y
* (((x, y))) => x + y
* [x, y] => x + y
* [x, ((y))] => x + y
* {x, y} => x + y

Actually - x, y => x + y would parse as x, (y => x + y) because ident followed by arrow is always an arrow function

I think we only need two value classifications - lvalue and rvalue - which can be combined to demonstrate different conditions:
* lv and rv = something like ident
* !lv and rv = MUST be an rval, can't be promoted to lval for any reason
* lv and !rv = MUST be an lval, can't be promoted to rval (eg object destructure with defaults, invalid as an rval)
* !lv and !rv = invalid combination, nothing should ever be this

Observation: Function declarations are almost never reassigned, and there's already an available sytnax for making a reassignable function (var x = function() {} or even var function() {}). Having function declarations be const by default enables a lot of optimizations without costing anything. Basically, type information can propagate without having to account for the underlying function being replaced.

Give var let semantics, let is more general than var and hoisting has limited usefulness. As a result, don't need "let" keyword

objects are instances of a prototype. They have a fixed shape based on their prototype, but can be extended arbitrarily - at which point they are transparently extended ala Python's __dict__. The core assumption is that objects created using a prototype aren't expecting their shape to change, but we want to enable that

new T(...args) = do => {
	let $x = object.create(T);
	return T::new($x, ...args) ?? $x;
}

function f(this, ...) {}
* lets you pass "this" as the first argument rather than implicitly. That is, if the function is called without "this", the first argument is used instead.

There's a complex taxonomy of types here which we have to wrangle somehow, particularly given polymorphic types have the same auto-coercing behaviors
* smi = 28 (?) bits on 32-bit, 63 bits on 64-bit
* heap integer = largest size available (possibly special case of long)
* long = arbitrary-precision integer
* These types should be transparent, presenting to user code with a single "int" type

Semantics here: Strict mode has strict typing, only allowing native types. To use variables from the enclosing scope, adding "as type" is mandatory. This expression throws a TypeError if the type of the operand isn't EXACTLY that type. The point of this, at least in this context, is to define the polymorphic operators in terms of the native operators which require known types and for both operands to be the same type, thus detangling it from whatever language or JIT library is being used to implement the language.

Native types include:
* any (must be explicit, operators aren't respected *including* indexing anything except proto and size)
  - supported operations: .proto .size is as === !==
* bool u8 i8 u16 i16 u32 i32 u64 i64 f32 f64
* dict
* struct
* type[]
* (a, b, c) tuple

Note that objects are forbidden, they must be passed as a struct (which only supports indexing operations)

Idea: 4 levels of code, asm, strict, static, and dynamic.
* asm uses a generic but consistent assembler syntax which compiles to a simple bytestring interpolation - it isn't executed as-is, but rather evaluates to a bytestring which is then inserted into the appropriate memory.
  - I played around with making such an assembly syntax a few years ago, maybe look for those notes?
  - If we go this route a common idiom might be generators which yield assembly bytestrings, which would look like `yield asm { ... }`
  - Though this wouldn't take into consideration the target dependence of assembly, maybe `asm { x86 { ... } arm { ... } powerpc { ... } wasm { ... } }`
* strict provides a C-like strict typing with espresso syntax. This level should be sufficient for JIT, as everything should eventually evaluate to it
* static is the normal modality for espresso and is mostly restricted to having static scoping
* dynamic to refer to a fourth level of code which has dynamic scope bindings as with REPL. This is implemented by transparently turning variable lookup into a dict lookup of the dynamic namespace object. If a variable is undefined, it must come from one of the enclosing dynamic scopes. Explict global names can be collated into slots for the object

It's unclear to me what needs to be done about the asm level, too tired I guess. At minimum it needs to support strict code, but there's certain constructs which just don't boil down to strict code:
* boxed values can't be defined by the language itself
* don't have a way to define non-strict functions in terms of strict code

To compile a self-hosting interpreter we'd want all (some?) of the code to be compiled to be in strict mode, which ensures that there isn't any runtime ambiguity because it disallows implicit boxing. To self-host it would also need a notion of what machine code to emit, which is where asm comes in. Asm being a construct only available to the top level of execution, lest we start breaking safety. Which is odd, because it sort of uses a superset of the language to ease development - parsing asm expressions would only be done for the top level, but not subsequent levels

Alternatively we could eschew explicit asm statements for a higher level bytecode which a lot of JITs seem to do. Have language constructs evaluated to a lower level code with strict semantics and then that code is sent to be assembled. Which is just strict mode, right? Writing a JIT we could just worry about strict mode code and use it as a platform for the higher level code, using strict mode as a high level intermediate language. But then how do we translate higher concepts to strict mode? Like boxed values

Strict mode represents boxed values with the opaque type "any" - the actual implementation of the type is completely unspecified, only defining a few operations for type introspection and explicit conversion. So, the format of any is up to the backend as it should be since it has a better idea of the limitations of boxing, eg 64-bit backends can use NaN-boxing instead of pointer tagging which gives more inline values

Maybe the "as" operator could return an intermediate value which is pre-offset into the value?

Objects have the following format:
Dict* shape (comparable to vptr)
HeapValue slots[]

A union can be considered as:
Dict* shape
HeapValue slots0[]
HeapValue slots1[]
...

We only need one shape for the whole object (unlike multiple inheritance in C++) because it's arbitrarily malleable. This does add the caveat, however, that methods called on `this` from one of the parents need an offset into the slots corresponding to the subobject - no this is incorrect, slot resolution can be done at prototype creation, and prototype lookup is still proxied by ICs

Design considerations for strict mode:
* Static AOT compilation
* No type inference - all types must be explicit
* Implement native and performance-critical code using the same syntax

Difficulties with strict mode:
* How do we implement the GC when the system itself is supposed to rely on it?

Take a page from C#, have an "unsafe" context as well as "strict" (which C# doesn't have, it's always strict) - unsafe enables pointer types and operations

Unsafe semantics:
* An unsafe block must be in an unsafe function
* A file containing unsafe code MUST be imported using "unsafe import"

function* fib(n: i32): i32 {
	var a = 0, b = 1;
	while(b < n) {
		yield b;
		a, b = b, a + b;
	}
}

Adding the i32 type annotation acts as an extra level of type hinting. A boxed value is passed to the function, which then checks the type is int and fits in 32 bits. The rest of the code can then assume an unboxed value, which may need reboxing if passed to non-strict functions.

proto is mutable, struct is not. Struct is defined by its definition, proto is more or less a bag of methods with a hint at how much to allocate

Strict function requirements:
* All types must be strict
  - i8 u8 i16 u16 i32 u32 i64 u64 f32 f64 (machine types)
  - any (implementation-defined variant type)
  - struct (machine type aggregate)
  - bytes buffer
  - aggregate types (safely encapsulated pointers)
    * list (resizeable)
	* tuple (immutable)
	* object (associative array with cacheable lookups)

tuple:gco(items:any[])
list:gco(items:*gco)
dict:gco(u32, u32, proto:any, indices:u8[], items:any[])
object:gco(shape:*dict, slots:any[])

object
* Direct lookup (eg ob["property"])
* Slot lookup (may fail on non-slotted entries)

we want a way to include unsafe code in safe code in a way which is safe

For example, suppose we wanted to implement Buffer
* Security risk: if a file can contain unsafe code without realizing
  - But also, we want to be able to permit unsafe code without requiring it - like eg importing a library which has an unsafe dependency. Buffer is a good example of that, or any other native data structure implementation that isn't made of existing primitives. Having the dependency would need to poison everything that depends on that even indirectly

Node gets around this with .node binaries, which must be precompiled. It doesn't allow arbitrary native code to execute because a compilation step must be executed first.

C# gets around this because it's a compiled language, even though it runs on a VM. When unsafe code (same semantics) is included in a module, it must be compiled with an /unsafe flag to permit its use. But, once the file is compiled it's considered "safe" and can be linked to other files without issue.

So let's do this - "unsafe import" is still a thing, and it specifies that the file to be imported may contain unsafe code, mostly used for rapid prototyping. For actual libraries however, they can be AOT compiled into object files which are treated as "safe". The extra compilation step is considered part of the safety, where arbitrary machine code can't be run without effort to enable it. If a unit is compiled, we know it's either safe or was compiled with the /unsafe flag

Ambiguities which come with relaxed restrictions for syntax:

Basic pattern for functions is "function" [ident] [lvalue] block. Main thing I was concerned with was function x y { ... } which would be valid but difficult to blacklist, made more complicated by short method syntax:
ident [lvalue] block

doThing {
	
}

"function" [ident] lvalue block

Open question is how permissive we want to be - should parameters always be surrounded by parentheses, or should we make it any lvalue?

Methods use a shorter format without the function keyword

add(x, y) {}
add[x, y] {}
add{x, y} {}
 -> for explicit functions, make parentheses mandatory. 
 
function x y {
	
}

Everything is an expression, but there is still a notion of statements - these are used to distinguish between automatic returns and differences of semantics. So for instance, without the concept of statements the following would return an instantiated generator:

function test() {
	for(var i = 0; i < 10; ++i) {
		print(i);
	}
}

In general this is really bad because it makes functions "leaky" - if there isn't an explicit return, instead of returning none (like one would expect) it returns whatever structure you were last working with.

Statements are a recusively defined property. Expressions cannot contain statements, but statements can contain expressions. This recursion stops at the top of an enclosing function's scope (or global scope). A function scope is considered a series of statements. If it's an expression, all subexpressions are expressions and not statements. Constructs which can be statements include:

Control structures: if then else loop while for switch try
Control flow: return fail break continue redo goto (these all have type `never`)
* yield does have a type but counts as a statement
Assignments: any mutating operator, x = y, x += y, ++y, etc

Basically anything with side effects. This definition would include function calls, which is somewhat problematic... so have a "none" return type ignore any implicit returns

There are also empty statements, as in ;; which forward the previous value. If there is no previous value eg (;) then `none` is assumed. Note that this is not an empty tuple because the inclusion of the semicolon operator makes this a sequential evaluation.

While implementing the parser I came across a nasty bit of ambiguity. The ++ and -- operators are the only pure unary operators which have side effects. Ordinarily the binding of unary operators with the previous line isn't much of an issue because expressions don't appear on their own, or lhs of an assignment. Eg +x = 10 isn't ever going to happen because +x is rvalue, and +x alone won't happen because it has no side effects. But for ++ and --, if the prefix form is used right after a statement it will usually coerce the statement into an expression with postfix form.

if(cond) { xxx }
++x;

will parse as (if(cond) { xxx} ++) x; because the parser is greedy and we usually don't append semicolons to blocks

So we may need to make some expression type distinctions. This is an unlikely valid parse, but one we can't rule out either because we could imagine something like

if(cond) { x } else { y }++ which conditionally increments either x or y (the if expression is itself an lvalue)

So naive distinction, expression v statement. Certain operators like ++ and -- may be disallowed from binding to statements, but statements can always be made into expressions by wrapping them in parentheses

Another distinction we want to make though is auto-returning. Loops in expression form create generators, which would be very bad if they were the last statement in a function body. Then the generator would be returned, probably ignored, and the loop would remain unexecuted. On the other hand there are statements we'd want to return automatically like if-else and switch.

In JS this is called "statement completion value". Statements which aren't valid expressions evaluate (literally within eval) to a special "empty" value which forwards earlier values. So eval("var x = 10; x = 4; var y = 30") == 4, because variable declarations are empty and the last non-empty expression was 4

Let's call the two types of statements completion and continuation

Expression
	Statement
		Continuation (loop while for with)
		Completion (if switch try )